{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLBootcamp21_Assignment_Week4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhuymJldwQq8"
      },
      "source": [
        "## MLBootcamp21 Week 4 Assignment: Gridsearch and CrossValidation\n",
        "- This week we will focus on improving our existing models using these two techniques\n",
        "- They are good approaches to improving your model's performance apart from feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtQ2vhXkwptR"
      },
      "source": [
        "## Q1. Use GridSearchCV and also explore other GridSearch techniques provided by scikit-learn to tune your hyperparameters and obtain the best model\n",
        "\n",
        "- Try out Decision Trees, Random Forest, GradientBoost, AdaBoost and also XGBoost \n",
        "- Run the model on the features you have generated until now without tuning the parameters first to check the result on basic parameters\n",
        "- Then apply GridSearchCV or other GridSearch techniques provided by scikit-learn to tune the hyperparameters and get results on the best model\n",
        "\n",
        "## Keep the \"random_state\" number as 42 or anynumber of your choice and report that number for me to be able to reproduce the same results\n",
        "\n",
        "- Report your performance on the test set after making the submission on kaggle. \n",
        "\n",
        "- ****Do not use some random existing notebook on Kaggle to get the best results as you will not learn anything that way and we will be able to easily know if that has been done. Do whatever you can****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmFiP4zSwo5X"
      },
      "source": [
        "# Write your code from this cell\n",
        "# It need not be in a single cell"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfx1tUtmykXK"
      },
      "source": [
        "## Q2. There might be times when you would only like to do K-Fold Cross Validation and not run the time consuming GridSearch everytime. That is what you will be doing in this question. \n",
        "\n",
        "- Read the documention for K-Fold crossvalidation provided by scikit-learn here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "\n",
        "- Try to understand and run the code provided by scikit-learn. Basically this function provided by scikit-learn will split your dataset into the \"K\" folds and will give you the indices of the \"K\" folds\n",
        "\n",
        "- Then use the K-Fold crossvalidation technique to generate the \"K\" folds and report the accuracy obtained for each of the \"K\" folds\n",
        "\n",
        "- You should get \"K\" different accuracy values and then finally take the average of all the \"K\" accuracies which would be your final model performance\n",
        "\n",
        "- This exercise will help you really understand K-Fold CrossValidation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp7w1n-DyjpI"
      },
      "source": [
        "## Write your code from here"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP1FogY_zrTk"
      },
      "source": [
        "## Thats it for this week. There are only two questions but they are a bit time consuming so enjoy! I hope you reach atleast 85-90% on Kaggle before tapping into Deep Learning"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}